[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOn Grounding\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText prompt-based Image Masking using Stable Diffusion: A series of experiments\n\n\n\n\n\n\nCode\n\n\nAI\n\n\nStableDiffusion\n\n\nfast.ai\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nJames Emilian\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello :)",
    "section": "",
    "text": "I am a computer vision researcher and biomedical engineer with expertise in developing multimodal models and finetuning foundation models for on-edge deployment on robots, and in clinical settings. I am currently a core ML team member at the DARPA Triage Challenge, developing multimodal deep learning models for battlefield trauma detection. The bulk of my work at CMU has centered around this role - rapid testing and deployment of algorithms for physiological sign sensing using robots. Given our aggressive deployment-first approach, I have more cool videos to show than NeurIPS publications - see them here.\nAside from my work on multimodal algorithms for medical triage robotics, I have worked on the below topics:\n- Machine Unlearning for privacy-preserving AI\n- Imitation Learning for long-horizon task planning in embodied AI simulations\n- Uncertainty-aware segmentation models for tumor and pleural effusion segmentation\nüìå Check out my Projects!!\n‚úçÔ∏è For my thoughts on AI & research, visit the Blog.\n\n\n\n\n\nJ on the rocks."
  },
  {
    "objectID": "darpa.html",
    "href": "darpa.html",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "ML Algorithms Core Member, CMU Robotics Institute\n\n\nWe developed multimodal deep learning models for stand-off vital sensing and hemorrhage detection in battlefield scenarios.\n\n\n\n\nBuilt CNN + Transformer hybrid models for heart and respiratory rate estimation.\nDeployed on NVIDIA Jetson Orin for real-time inference.\nField-tested on diverse robotic platforms for DARPA Challenge Year 1.\nManuscript awarded Best Thesis Award.\n\n\n\n\n\nDeployment on Jetson Orin\nRobotic Platform Field Test\n\n\nüîô Back to Projects"
  },
  {
    "objectID": "darpa.html#overview",
    "href": "darpa.html#overview",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "We developed multimodal deep learning models for stand-off vital sensing and hemorrhage detection in battlefield scenarios."
  },
  {
    "objectID": "darpa.html#research-contributions",
    "href": "darpa.html#research-contributions",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "Built CNN + Transformer hybrid models for heart and respiratory rate estimation.\nDeployed on NVIDIA Jetson Orin for real-time inference.\nField-tested on diverse robotic platforms for DARPA Challenge Year 1.\nManuscript awarded Best Thesis Award."
  },
  {
    "objectID": "darpa.html#video-demonstrations",
    "href": "darpa.html#video-demonstrations",
    "title": "DARPA Triage Challenge",
    "section": "",
    "text": "Deployment on Jetson Orin\nRobotic Platform Field Test\n\n\nüîô Back to Projects"
  },
  {
    "objectID": "posts/g_dino_explained/index.html",
    "href": "posts/g_dino_explained/index.html",
    "title": "On Grounding",
    "section": "",
    "text": "Why this post? Well - as a computer vision researcher working in health sensing, I stumbled into the concept of grounding out of sheer necessity. Without revealing too much, I‚Äôll just say that my grant required me to solve a tough medical problem where both false positives and false negatives were heavily penalized. The naive vision model I used had a tendency towards false positives, and I soon discovered that I could used visually grounded models to reduce the tendency for false positives by bringing in a granular ‚Äòworld knowledge‚Äô.\nwith all this hype around grounding; and as someone who merely chanced into it in the process of solving a pressing deployment problem, I feel compelled to understsnd this concept. It‚Äôs fascinating how, for example G-DINO is able to do this, and I want to understand this as deeply as I can."
  },
  {
    "objectID": "posts/DiffEditMask/index.html",
    "href": "posts/DiffEditMask/index.html",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "",
    "text": "Inspired by the amazing paper ‚ÄúDiffEdit: Diffusion-based semantic image editing with mask guidance‚Äù which Jeremy discussed in last week‚Äôs class of Practical Deep Learning for Coders Pt2, this is an attempt (v1) at implementing step one of the paper‚Äôs semantic image editing method.\nSee below an image from the paper, illustrating this novel method of editing images simply using a text query:\n\nSeeing the automatic mask generation was fascinating for me, and it was evident from interactions during the class that this was indeed a novel, interesting way to generate pixel masks. Stable Diffusion, the poster-child for modern AI, and the basis for cool tech like Dream Studio and Midjourney, could do something more pragmatic too! The mask generation strategy is simple - it involves using stable diffusion to create a pixel mask for an input image, simply using two text prompts. Instead of going off on an verbose explanation I‚Äôll allow a screenshot from the DiffEdit paper to do the job. \n\nIf you wish to delve deeper into Stable Diffusion I highly recommend going through this amazing repo from fast.ai - diffusion nbs on GitHub.\nTo quote the paper, for the mask creation - ‚ÄúWe add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results‚Äù. The above screenshot is part of a detailed schematic - where the input is the image x0 and query text Q = 'Zebra' and reference text R = 'Horse'. The StableDiffusionImg2ImgPipeline pipeline from HuggingFace takes care of adding the Gaussian noise to an input image, and denoising the resulting image by running inference using a pretrained model. We can play around with the inference input parameters like input image, number of inference steps, strength of prompt guidance, etc. to influence the denoised image output.  Note: This post documents multiple experiments, with varying levels of quality of final output. The intent is to capture the thought process and important trial and error and resulting insights that went into trying to generate a good pixel mask using SD.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nLoad the pretrained StableDiffusionImg2ImgPipeline onto pipe:\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",revision=\"fp16\",torch_dtype=torch.float16, safety_checker = None).to(\"cuda\")\n\nLet‚Äôs load and take a look at the input image, x0 :\n\ninit_image = Image.open(\"dark-horse-riverV2.jpeg\")\npixels = init_image.load()\ninit_image.show()"
  },
  {
    "objectID": "posts/DiffEditMask/index.html#introduction-setup",
    "href": "posts/DiffEditMask/index.html#introduction-setup",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "",
    "text": "Inspired by the amazing paper ‚ÄúDiffEdit: Diffusion-based semantic image editing with mask guidance‚Äù which Jeremy discussed in last week‚Äôs class of Practical Deep Learning for Coders Pt2, this is an attempt (v1) at implementing step one of the paper‚Äôs semantic image editing method.\nSee below an image from the paper, illustrating this novel method of editing images simply using a text query:\n\nSeeing the automatic mask generation was fascinating for me, and it was evident from interactions during the class that this was indeed a novel, interesting way to generate pixel masks. Stable Diffusion, the poster-child for modern AI, and the basis for cool tech like Dream Studio and Midjourney, could do something more pragmatic too! The mask generation strategy is simple - it involves using stable diffusion to create a pixel mask for an input image, simply using two text prompts. Instead of going off on an verbose explanation I‚Äôll allow a screenshot from the DiffEdit paper to do the job. \n\nIf you wish to delve deeper into Stable Diffusion I highly recommend going through this amazing repo from fast.ai - diffusion nbs on GitHub.\nTo quote the paper, for the mask creation - ‚ÄúWe add noise to the input image, and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results‚Äù. The above screenshot is part of a detailed schematic - where the input is the image x0 and query text Q = 'Zebra' and reference text R = 'Horse'. The StableDiffusionImg2ImgPipeline pipeline from HuggingFace takes care of adding the Gaussian noise to an input image, and denoising the resulting image by running inference using a pretrained model. We can play around with the inference input parameters like input image, number of inference steps, strength of prompt guidance, etc. to influence the denoised image output.  Note: This post documents multiple experiments, with varying levels of quality of final output. The intent is to capture the thought process and important trial and error and resulting insights that went into trying to generate a good pixel mask using SD.\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nLoad the pretrained StableDiffusionImg2ImgPipeline onto pipe:\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\",revision=\"fp16\",torch_dtype=torch.float16, safety_checker = None).to(\"cuda\")\n\nLet‚Äôs load and take a look at the input image, x0 :\n\ninit_image = Image.open(\"dark-horse-riverV2.jpeg\")\npixels = init_image.load()\ninit_image.show()"
  },
  {
    "objectID": "posts/DiffEditMask/index.html#single-output---based-mask-generation",
    "href": "posts/DiffEditMask/index.html#single-output---based-mask-generation",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "Single output - based mask generation",
    "text": "Single output - based mask generation\nIn this case, we‚Äôll keep num_images_per_prompt=1 in the image-image stable diffusion pipeline, and see how good a mask we can generate using that. We‚Äôll play around a bit with num_inference_steps to create the best possible mask.\npipe takes care of adding noise to the input image init_image, and denoising it for num_inference_steps=8, with respect to the reference text - \"a horse\".\n\ntorch.manual_seed(1000)\nprompt = \"a horse\" #Denoising image with respect to the identifying phrase \nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\nThat‚Äôs a scary looking horse, one must admit! For some reason I‚Äôve seen from my experiments (not recorded here) that the model often grows extra appendages from the tail pixels of horse images. We can try reducing num_inference_steps to prevent that, but the downside is that the output image will be noisy, since we haven‚Äôt allowed for enough inference steps. For now, we‚Äôll move on and see what happens with the query text denoising.\nWe use pipe to add noise to the input image init_image, and denoise it for num_inference_steps=10, with respect to the query text - \"a zebra\".\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nThat was unexpected! Let‚Äôs reduce num_inference_steps to 8 to prevent the inference from going too far!\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nBetter. Notice how the background is changing considerably based on the prompt. Let‚Äôs try altering the prompt to make sure the backgrounds are relatively similar in the images denoised with respect to both Query Q and Reference R.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nNotice how giving more information in the prompt about the action of the subject with the input ‚Äúa zebra drinking water‚Äù has allowed the model to run for 10 inference steps without morphing the zebra too badly!\nNeither of those are particularly pretty images on their own; but let‚Äôs attempt to find the ‚Äúnormalised difference between the denoised images‚Äù, and generate a rough mask. Then we can try to play with num_inference_steps and create a better mask. Once that parameter‚Äôs ideal value is clear we can move to trying to use multiple denoised images.\n\nfrom torchvision import transforms\nconvert_to_tensor = transforms.ToTensor()\nconvert_to_image = transforms.ToPILImage()\n\nSince we‚Äôll be using this operation often, let‚Äôs define a function to compute the normalised absolute difference between two images.\n\ndef norm_diff_abs(a,b):\n    a_tens = convert_to_tensor(a)\n    b_tens = convert_to_tensor(b)\n    ftens = (abs(a_tens-b_tens))/(a_tens+b_tens)\n    fimage = convert_to_image(ftens)\n    return fimage\n\nNote that abs() is an important part of getting a meaningful normalised difference image. Without it the resulting difference image doesn‚Äôt serve our purpose well. Ponder the reason, dear reader ;)\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\n\n\nim_diff_normabs\n\n\n\n\n\n\n\n\nNow that we have the normalised difference between the denoised images (a visual representation of the contrast between reference text and query), let‚Äôs write a function to convert this image to grayscale and binarize this. This function, when called, should yield the final mask M.\n\ndef GSandBinarize(im, **kwargs):\n    im_array = asarray(im)\n    im_grayscale = cv2.cvtColor(im_array, cv2.COLOR_BGR2GRAY)\n    if kwargs[\"thresh_method\"] == 'manual':\n        th, im_binary_array = cv2.threshold(im_grayscale, kwargs[\"thresh\"], 255, cv2.THRESH_BINARY) #manual thresholding\n    else:\n        th, im_binary_array = cv2.threshold(im_grayscale, 0, 255, cv2.THRESH_OTSU) #auto thresholding\n    im_binary = Image.fromarray(im_binary_array)\n    return im_binary      \n\nNow let‚Äôs try binarising the difference using the OTSU thresholding method, which will automatically pick a pixel threshold value.\n\nGSandBinarize(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\nThe backgound is not too shabby; it‚Äôs almost fully blacked out, but the horse body has a lot of black areas. Now let‚Äôs try binarising the difference using the binary thresholding method, which allows us to pick a pixel threshold value.\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nNow, in this case, more of the horse‚Äôs body is white, making the mask better at capturing the subject; but lowering the pixel threshold manually has given rise to many white patches in the background as well. Let‚Äôs try solving this.\nSideNote: For the exact same model, random seed, and parameters, the mask seems to come out slightly better on a A4000 GPU, as compared to a P5000. All outputs shown here are from running inference on a Paperspace A4000 instance.\nNow that we have the whole mask generation process ready, let‚Äôs play a bit with the num_inference_steps parameter to see if we can get a better mask!! Let‚Äôs start with num_inference_steps=6.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase (lower number of inf steps)\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase (lower number of inf steps)\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\nGSandBinarise(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nAs we can see, this leads to a much clearer outline of the horse than before, but the image is littered with too many white patches. I suspect that that can be solved by averaging out multiple outputs (see next section), and so num_inference_steps=6 can be considered a good candidate for our final run. Now let‚Äôs try num_inference_steps=8:\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\nGSandBinarise(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nWell why not use a higher num_inference_steps to allow for more denoising? Fair point; but what often seems to happen is that, weird artifacts are generated at a high value of num_inference_steps :\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\" #Denoising image with respect to the identifying phrase\nimage_q = pipe(prompt=prompt_q, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=30).images\nimage_q[0]\n\n\n\n\n\n\n\n\n\n\n\nA super clean-looking image, but now the zebra has two heads!\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\" #Denoising image with respect to the identifying phrase\nimage = pipe(prompt=prompt, num_images_per_prompt=1, init_image=init_image, strength=0.8, num_inference_steps=30).images\nimage[0]\n\n\n\n\n\n\n\n\n\n\n\n\nim_diff_normabs = norm_diff_abs(image_q[0], image[0])\nGSandBinarise(im_diff_normabs, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nGSandBinarise(im_diff_normabs, thresh_method='manual', thresh=30)\n\n\n\n\n\n\n\n\nIt‚Äôs still just as noisy as the mask outputs we got using a much lower value of num_inference_steps, and now, the outline has become less accurate because of all the extra heads in the denoised images! Let‚Äôs stick to a lower value for num_inference_steps and come up with a different way to get a clearer mask."
  },
  {
    "objectID": "posts/DiffEditMask/index.html#multi-output---based-mask-generation",
    "href": "posts/DiffEditMask/index.html#multi-output---based-mask-generation",
    "title": "Text prompt-based Image Masking using Stable Diffusion: A series of experiments",
    "section": "Multi output - based mask generation",
    "text": "Multi output - based mask generation\nSince we are denoising a image after adding Gaussian noise to it, we could try to generate multiple denoised images for each prompt, and average them out before taking the normalised difference of query versus reference. The idea is that averaging multiple images denoised based on the same prompt will help cancel out the random noise, and also create a clearer image of the model‚Äôs idea of a ‚Äúzebra‚Äù or ‚Äúhorse‚Äù.  Note: Judging from the DiffEdit paper‚Äôs schematic of the process, this is likely the method adopted by its authors.\n\npipe.enable_attention_slicing()\n\n\n# dark-horse-riverv2-rsz\ninit_image = Image.open(\"dark-horse-riverV2-rsz.jpg\")\npixels = init_image.load()\ninit_image.show()\n\n\n\n\n\n\n\n\nThe image has been cropped and resized to avoid a CUDA OOM error when running for a high value of num_images_per_prompt.\nFirst, we try to implement this with 10 images per prompt, denoised for num_inference_steps=6.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r6 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage_grid(images_r6, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q6 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=6).images\nimage_grid(images_q6, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs build out the function for passing in the list of images; and getting the averaged out image! How? Stack the list of images generated and create an average of them, along the ‚Äúnumber of images‚Äù axis.  Note - This function, like all others, was built by experimenting with each line of its components and then putting them together - a nifty trick, courtesy of Jeremy!\n\ndef get_averageIm(ImList):\n    imtensors = []\n    for Im in ImList:\n        imtensor = convert_to_tensor(Im)\n        imtensors.append(imtensor)\n        \n    init_tensor = torch.zeros(imtensors[0].shape)\n    for tensor in imtensors:\n        init_tensor += tensor\n        \n    total_tensor = init_tensor\n    av_imtensor = (total_tensor)/10\n    av_image = convert_to_image(av_imtensor)\n    \n    return av_image\n\n\nav_image_r6 = get_averageIm(images_r6)\nav_image_r6\n\n\n\n\n\n\n\n\n\nav_image_q6 = get_averageIm(images_q6)\nav_image_q6\n\n\n\n\n\n\n\n\nLike it or not, this is what peak ‚Äúhorse‚Äù and ‚Äúzebra‚Äù performance look like! (reference - Chapter 4 of fastbook).\n\nim_diff_normabs6 = norm_diff_abs(av_image_r6, av_image_q6)\nGSandBinarise(im_diff_normabs6, thresh_method='OTSU')\n\n\n\n\n\n\n\n\n\nmask_6step = GSandBinarise(im_diff_normabs6, thresh_method='manual', thresh=20)\nmask_6step\n\n\n\n\n\n\n\n\nRepeat the same for other num_inference_steps=8 values to get a better mask. Also remember to play around with manual threshold value, to get ideal mask. \nThe mask output looked decent for num_inference_steps=8 in the single-output section, so let‚Äôs try that here:\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r8 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_grid(images_r8, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q8 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=8).images\nimage_grid(images_q8, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\nav_image_r8 = get_averageIm(images_r8)\nav_image_r8\n\n\n\n\n\n\n\n\n\nav_image_q8 = get_averageIm(images_q8)\nav_image_q8\n\n\n\n\n\n\n\n\n\nim_diff_normabs8 = norm_diff_abs(av_image_r8, av_image_q8)\nmask_otsu_8step = GSandBinarise(im_diff_normabs8, thresh_method='OTSU')\nmask_otsu_8step\n\n\n\n\n\n\n\n\n\nmask_8step = GSandBinarise(im_diff_normabs8, thresh_method='manual', thresh=25)\nmask_8step\n\n\n\n\n\n\n\n\nThat looks better than the previous mask. We‚Äôve considerably reduced the white patches in the background. Let‚Äôs try num_inference_steps=7.\n\n#inf steps 7\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r7 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=7).images\nimage_grid(images_r7, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q7 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=7).images\nimage_grid(images_q7, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\nav_image_r7 = get_averageIm(images_r7)\nav_image_r7\n\n\n\n\n\n\n\n\n\nav_image_q7 = get_averageIm(images_q7)\nav_image_q7\n\n\n\n\n\n\n\n\n\nim_diff_normabs7 = norm_diff_abs(av_image_r7, av_image_q7)\nmask_otsu_7step = GSandBinarise(im_diff_normabs7, thresh_method='OTSU')\nmask_otsu_7step\n\n\n\n\n\n\n\n\n\nmask_7step = GSandBinarise(im_diff_normabs7, thresh_method='manual', thresh=25)\nmask_7step\n\n\n\n\n\n\n\n\nThis mask (num_inference_steps=7) has a slightly cleaner outline than the one for num_inference_steps=8. Let‚Äôs try going a bit higher; num_inference_steps=10\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r10 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_grid(images_r10, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q10 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=10).images\nimage_grid(images_q10, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\nAs one can see, there are a few weird images, and all the images are quite deformed with respect to the original init_image. Let‚Äôs look at the generated mask.\n\nav_image_r10 = get_averageIm(images_r10)\nav_image_r10\n\nav_image_q10 = get_averageIm(images_q10)\nav_image_q10\n\n\n\n\n\n\n\n\n\nim_diff_normabs10 = norm_diff_abs(av_image_r10, av_image_q10)\nmask_otsu_10step = GSandBinarise(im_diff_normabs10, thresh_method='OTSU')\nmask_otsu_10step\n\n\n\n\n\n\n\n\n\nmask_10step = GSandBinarise(im_diff_normabs10, thresh_method='manual', thresh=25)\nmask_10step\n\n\n\n\n\n\n\n\nWe start to see a more patchy outline, because the denoised images have deviated more and more from the original outline. Let‚Äôs go down one step and try num_inference_steps=9.\n\ntorch.manual_seed(1000)\nprompt = \"a horse drinking water\"\nimages_r9 = pipe(prompt=prompt, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=9).images\nimage_grid(images_r9, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1000)\nprompt_q = \"a zebra drinking water\"\nimages_q9 = pipe(prompt=prompt_q, num_images_per_prompt=10, init_image=init_image, strength=0.8, num_inference_steps=9).images\nimage_grid(images_q9, rows=2, cols=5)\n\n\n\n\n\n\n\n\n\n\n\nThe images are still weird and vary considerably in the position of the subject, so we can expect the mask to be ‚Äúnoisy‚Äù as well.\n\nav_image_r9 = get_averageIm(images_r9)\nav_image_r9\n\n\n\n\n\n\n\n\n\nav_image_q9 = get_averageIm(images_q9)\nav_image_q9\n\n\n\n\n\n\n\n\n\nim_diff_normabs9 = norm_diff_abs(av_image_r9, av_image_q9)\nmask_otsu_9step = GSandBinarise(im_diff_normabs9, thresh_method='OTSU')\nmask_otsu_9step\n\n\n\n\n\n\n\n\n\nmask_9step = GSandBinarise(im_diff_normabs9, thresh_method='manual', thresh=30)\nmask_9step\n\n\n\n\n\n\n\n\nThough better than the previous run, this still has a rather unclear outline. From the above analyses, we can pick num_inference_steps=6, 7, 8 as yielding usable pixel masks.\n\nmask_list = [mask_6step, mask_7step, mask_8step]\nimage_grid(mask_list, rows=1, cols=3)\n\n\n\n\n\n\n\n\nIf we could see how these masks look on the original image, I suppose they‚Äôd be more convincing. I‚Äôm currently facing a minor bug in overlaying a red mask onto the original image; and this notebook will be updated with the same once that is resolved.\nThough a far cry from the clean mask illustrated in the DiffEdit paper, I suppose this isn‚Äôt a bad start. As I experiment with other techniques (like using other noise schedulers) to generate a fully clean mask, I‚Äôll be adding relevant updates on this post. \nHopefully you got something from your time here! If you read this and have any suggestions/comments on how to improve my code (or words!), please reach out to me @Twitter. \nCheers!! üòÑ"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Development and Deployment of Remote Health Sensing Models\n\n‚ÄúWhen we have a significant emergency where there could be danger to the responders, you can save lives by not sending in a human right away. This autonomous triage system allows us to provide rapid help to as many victims as possible without needing extra human power ‚Äî an essential gap to fill when medics are limited.‚Äù\n‚Äî Dr.¬†Lenny Weiss, Professor of Emergency Medicine, UPitt; Medical Director of City of Pittsburgh Police-SWAT\n\n \n\n\n\n\n\n\n\nDeveloped an ARKit-based system to assess ocular alertness via blink rate tracking. \n\n\n\n\n\nLed testing across classical spectral-reflectance models and transformer-based architectures.\nFinal model deployed: a 3D CNN-based system, benchmarked extensively on custom IRB-approved datasets.\n\nüìπ Demo\n  \n\n\n\n\nConducted extensive benchmarking of segmentation models and Vision-Language Models (VLMs).\nFinal deployment: A segmentation model detecting severe hemorrhage presence in real-time robotic data streams.\n\nüìπ Live Robotic Data Demo\n   \n\nThis research opened a new problem space‚Äîachieving grounding in small models for data-scarce tasks.\nAs we tackle the much harder challenge of performing hemorrhage detection in further degraded sensing environments for Year #2 Challenge (daytime operations in thick smoke, nightime operations with no visible illumination, etc.) we are continuing to investigate - (i) how do we achieve strong visual grounding in unstructured environments despite the scarcity of relevant data? and (ii) how do we use multimodal data streams - like multispectral imaging - to characterize hemorrhage presence even in absence of RGB illumination?"
  },
  {
    "objectID": "projects.html#darpa-triage-challenge---core-member-team-chiron",
    "href": "projects.html#darpa-triage-challenge---core-member-team-chiron",
    "title": "Projects",
    "section": "",
    "text": "Development and Deployment of Remote Health Sensing Models\n\n‚ÄúWhen we have a significant emergency where there could be danger to the responders, you can save lives by not sending in a human right away. This autonomous triage system allows us to provide rapid help to as many victims as possible without needing extra human power ‚Äî an essential gap to fill when medics are limited.‚Äù\n‚Äî Dr.¬†Lenny Weiss, Professor of Emergency Medicine, UPitt; Medical Director of City of Pittsburgh Police-SWAT"
  },
  {
    "objectID": "projects.html#key-contributions",
    "href": "projects.html#key-contributions",
    "title": "Projects",
    "section": "",
    "text": "Developed an ARKit-based system to assess ocular alertness via blink rate tracking. \n\n\n\n\n\nLed testing across classical spectral-reflectance models and transformer-based architectures.\nFinal model deployed: a 3D CNN-based system, benchmarked extensively on custom IRB-approved datasets.\n\nüìπ Demo\n  \n\n\n\n\nConducted extensive benchmarking of segmentation models and Vision-Language Models (VLMs).\nFinal deployment: A segmentation model detecting severe hemorrhage presence in real-time robotic data streams.\n\nüìπ Live Robotic Data Demo\n   \n\nThis research opened a new problem space‚Äîachieving grounding in small models for data-scarce tasks.\nAs we tackle the much harder challenge of performing hemorrhage detection in further degraded sensing environments for Year #2 Challenge (daytime operations in thick smoke, nightime operations with no visible illumination, etc.) we are continuing to investigate - (i) how do we achieve strong visual grounding in unstructured environments despite the scarcity of relevant data? and (ii) how do we use multimodal data streams - like multispectral imaging - to characterize hemorrhage presence even in absence of RGB illumination?"
  },
  {
    "objectID": "projects.html#seizure-classification-from-eeg-signals",
    "href": "projects.html#seizure-classification-from-eeg-signals",
    "title": "Projects",
    "section": "üß† Seizure Classification from EEG Signals",
    "text": "üß† Seizure Classification from EEG Signals\n\nDeveloped a novel framework for capturing inter-annotator variability (aleatoric uncertainty) in seizure detection classification using EEG data.   üìÑ Read the Publication"
  },
  {
    "objectID": "projects.html#uncertainty-aware-segmentation",
    "href": "projects.html#uncertainty-aware-segmentation",
    "title": "Projects",
    "section": "üé≠ Uncertainty-Aware Segmentation",
    "text": "üé≠ Uncertainty-Aware Segmentation\n\nDeveloped a framework for uncertainty-aware segmentation, in collaboration with a friend and a fellow researcher, Nishanth TA. \nResearch manuscript in progress."
  },
  {
    "objectID": "projects.html#get-in-touch",
    "href": "projects.html#get-in-touch",
    "title": "Projects",
    "section": "üì© Get in Touch",
    "text": "üì© Get in Touch\nIf you‚Äôre interested in working together or just want to chat science || books || music - feel free to reach out via LinkedIn, Twitter or Email."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi there!",
    "section": "",
    "text": "In a constant state of figuring. I‚Äôm in my element when I have to make things work in high-stakes settings. Love writing as an exercise in thought and imagination. As for leisure - I love to make time for my friends, cook, talk outlandish philosphy, and go climbing or lifting."
  }
]